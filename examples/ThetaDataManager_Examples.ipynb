{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ThetaData Synchronization Manager - Examples\n",
    "\n",
    "**Version:** 1.0.9  \n",
    "**Last Updated:** December 2025\n",
    "\n",
    "This notebook demonstrates practical usage of tdSynchManager library for downloading, validating, and persisting market data from ThetaData API.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Imports](#setup)\n",
    "2. [Example 1: Basic EOD Download to CSV](#ex1)\n",
    "3. [Example 2: Multi-Symbol EOD Download](#ex2)\n",
    "4. [Example 3: Intraday Data to Parquet](#ex3)\n",
    "5. [Example 4: InfluxDB Integration](#ex4)\n",
    "6. [Example 5: Coherence Check & Recovery](#ex5)\n",
    "7. [Example 6: Custom Discovery Policy](#ex6)\n",
    "8. [Example 7: Querying InfluxDB Data](#ex7)\n",
    "9. [Example 8: Verify Data Completeness](#ex8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup & Imports\n",
    "\n",
    "Import required libraries and verify installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Import tdSynchManager components\n",
    "from tdSynchManager import ManagerConfig, ThetaSyncManager, ThetaDataV3Client\n",
    "from tdSynchManager.config import Task, DiscoverPolicy\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è Important**: Ensure your ThetaData API token is set as environment variable:\n",
    "\n",
    "```bash\n",
    "# Windows (PowerShell)\n",
    "$env:THETADATA_API_TOKEN=\"your_token_here\"\n",
    "\n",
    "# Windows (CMD)\n",
    "set THETADATA_API_TOKEN=your_token_here\n",
    "\n",
    "# Linux/macOS\n",
    "export THETADATA_API_TOKEN=\"your_token_here\"\n",
    "```\n",
    "\n",
    "Or use a `.env` file:\n",
    "\n",
    "```ini\n",
    "# .env\n",
    "THETADATA_API_TOKEN=your_token_here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex1'></a>\n",
    "## Example 1: Basic EOD Download to CSV\n",
    "\n",
    "Download daily (EOD) data for a single symbol to CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = ManagerConfig(\n",
    "    root_dir=\"./data\",        # Output directory\n",
    "    max_concurrency=5         # Parallel downloads\n",
    ")\n",
    "\n",
    "# Task definition\n",
    "tasks = [\n",
    "    Task(\n",
    "        asset=\"stock\",\n",
    "        symbols=[\"AAPL\"],\n",
    "        intervals=[\"1d\"],\n",
    "        sink=\"csv\",\n",
    "        first_date_override=\"20240101\",\n",
    "        end_date_override=\"20241231\",\n",
    "        discover_policy=DiscoverPolicy(mode=\"skip\")  # Don't auto-discover\n",
    "    )\n",
    "]\n",
    "\n",
    "# Run synchronization\n",
    "async def run_example1():\n",
    "    async with ThetaDataV3Client() as client:\n",
    "        manager = ThetaSyncManager(config, client=client)\n",
    "        await manager.run(tasks)\n",
    "\n",
    "# Execute\n",
    "await run_example1()\n",
    "print(\"\\n‚úÖ Example 1 completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify downloaded data\n",
    "import glob\n",
    "\n",
    "csv_files = glob.glob(\"./data/stock/AAPL/1d/csv/*.csv\")\n",
    "if csv_files:\n",
    "    df = pd.read_csv(csv_files[0], dtype=str)\n",
    "    print(f\"üìä Downloaded {len(df)} rows\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst 5 rows:\\n{df.head()}\")\n",
    "else:\n",
    "    print(\"‚ùå No CSV files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex2'></a>\n",
    "## Example 2: Multi-Symbol EOD Download\n",
    "\n",
    "Download EOD data for multiple symbols simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = ManagerConfig(\n",
    "    root_dir=\"./data\",\n",
    "    max_concurrency=10  # Higher concurrency for multiple symbols\n",
    ")\n",
    "\n",
    "# Multi-symbol task\n",
    "tasks = [\n",
    "    Task(\n",
    "        asset=\"stock\",\n",
    "        symbols=[\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\"],\n",
    "        intervals=[\"1d\"],\n",
    "        sink=\"csv\",\n",
    "        first_date_override=\"20240101\",\n",
    "        end_date_override=\"20241231\",\n",
    "        discover_policy=DiscoverPolicy(mode=\"skip\")\n",
    "    )\n",
    "]\n",
    "\n",
    "async def run_example2():\n",
    "    async with ThetaDataV3Client() as client:\n",
    "        manager = ThetaSyncManager(config, client=client)\n",
    "        await manager.run(tasks)\n",
    "\n",
    "await run_example2()\n",
    "print(\"\\n‚úÖ Example 2 completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex3'></a>\n",
    "## Example 3: Intraday Data to Parquet\n",
    "\n",
    "Download 5-minute intraday bars and save to Parquet format (compressed, faster queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Parquet\n",
    "config = ManagerConfig(\n",
    "    root_dir=\"./data\",\n",
    "    max_concurrency=3\n",
    ")\n",
    "\n",
    "# Intraday task (5-minute bars)\n",
    "tasks = [\n",
    "    Task(\n",
    "        asset=\"stock\",\n",
    "        symbols=[\"SPY\"],\n",
    "        intervals=[\"5min\"],\n",
    "        sink=\"parquet\",\n",
    "        first_date_override=\"20241201\",\n",
    "        end_date_override=\"20241215\",\n",
    "        discover_policy=DiscoverPolicy(mode=\"skip\")\n",
    "    )\n",
    "]\n",
    "\n",
    "async def run_example3():\n",
    "    async with ThetaDataV3Client() as client:\n",
    "        manager = ThetaSyncManager(config, client=client)\n",
    "        await manager.run(tasks)\n",
    "\n",
    "await run_example3()\n",
    "print(\"\\n‚úÖ Example 3 completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet file\n",
    "parquet_files = glob.glob(\"./data/stock/SPY/5min/parquet/*.parquet\")\n",
    "if parquet_files:\n",
    "    df = pd.read_parquet(parquet_files[0])\n",
    "    print(f\"üìä Parquet file contains {len(df)} rows\")\n",
    "    print(f\"\\nFirst 5 rows:\\n{df.head()}\")\n",
    "    \n",
    "    # Show file size advantage\n",
    "    import os\n",
    "    parquet_size = os.path.getsize(parquet_files[0])\n",
    "    print(f\"\\nüíæ File size: {parquet_size / 1024:.2f} KB\")\n",
    "else:\n",
    "    print(\"‚ùå No Parquet files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex4'></a>\n",
    "## Example 4: InfluxDB Integration\n",
    "\n",
    "Download data and write directly to InfluxDB for time-series analysis.\n",
    "\n",
    "**Prerequisites:**\n",
    "- InfluxDB 3.x running at http://localhost:8086\n",
    "- Valid InfluxDB token\n",
    "- Existing bucket (database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InfluxDB configuration\n",
    "influx_config = ManagerConfig(\n",
    "    root_dir=\"./data\",\n",
    "    max_concurrency=3,\n",
    "    influx_url=\"http://localhost:8086\",\n",
    "    influx_bucket=\"ThetaData\",\n",
    "    influx_token=\"your_influx_token_here\",  # Replace with your token\n",
    "    influx_measure_prefix=\"\",\n",
    "    influx_write_batch=5000\n",
    ")\n",
    "\n",
    "# Task for InfluxDB\n",
    "tasks = [\n",
    "    Task(\n",
    "        asset=\"stock\",\n",
    "        symbols=[\"SPY\"],\n",
    "        intervals=[\"1d\"],\n",
    "        sink=\"influxdb\",\n",
    "        first_date_override=\"20240101\",\n",
    "        end_date_override=\"20241231\",\n",
    "        discover_policy=DiscoverPolicy(mode=\"skip\")\n",
    "    )\n",
    "]\n",
    "\n",
    "async def run_example4():\n",
    "    async with ThetaDataV3Client() as client:\n",
    "        manager = ThetaSyncManager(influx_config, client=client)\n",
    "        await manager.run(tasks)\n",
    "\n",
    "# Uncomment to run (requires InfluxDB setup)\n",
    "# await run_example4()\n",
    "# print(\"\\n‚úÖ Example 4 completed!\")\n",
    "\n",
    "print(\"‚ö†Ô∏è Example 4 requires InfluxDB setup. See MANUAL.md Chapter 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex5'></a>\n",
    "## Example 5: Coherence Check & Recovery\n",
    "\n",
    "Enable automatic gap detection and recovery for existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with coherence checking enabled\n",
    "config = ManagerConfig(\n",
    "    root_dir=\"./data\",\n",
    "    max_concurrency=5,\n",
    "    coherence_mode=\"full\",  # Options: \"off\", \"light\", \"full\"\n",
    "    coherence_tolerance=0.05  # Allow 5% missing data before triggering recovery\n",
    ")\n",
    "\n",
    "# Task with coherence\n",
    "tasks = [\n",
    "    Task(\n",
    "        asset=\"stock\",\n",
    "        symbols=[\"AAPL\"],\n",
    "        intervals=[\"1d\"],\n",
    "        sink=\"csv\",\n",
    "        first_date_override=\"20240101\",\n",
    "        end_date_override=\"20241231\",\n",
    "        discover_policy=DiscoverPolicy(mode=\"skip\")\n",
    "    )\n",
    "]\n",
    "\n",
    "async def run_example5():\n",
    "    async with ThetaDataV3Client() as client:\n",
    "        manager = ThetaSyncManager(config, client=client)\n",
    "        await manager.run(tasks)\n",
    "\n",
    "await run_example5()\n",
    "print(\"\\n‚úÖ Example 5 completed!\")\n",
    "print(\"Check logs above for [COHERENCE] messages indicating gap detection/recovery.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex6'></a>\n",
    "## Example 6: Custom Discovery Policy\n",
    "\n",
    "Use different discovery policies to control symbol and date range behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ManagerConfig(\n",
    "    root_dir=\"./data\",\n",
    "    max_concurrency=5\n",
    ")\n",
    "\n",
    "# Example A: skip - No discovery, only specified symbols/dates\n",
    "task_skip = Task(\n",
    "    asset=\"stock\",\n",
    "    symbols=[\"AAPL\"],\n",
    "    intervals=[\"1d\"],\n",
    "    sink=\"csv\",\n",
    "    first_date_override=\"20240101\",\n",
    "    end_date_override=\"20240131\",\n",
    "    discover_policy=DiscoverPolicy(mode=\"skip\")\n",
    ")\n",
    "\n",
    "# Example B: mild_skip - Discover new symbols, keep existing dates\n",
    "task_mild = Task(\n",
    "    asset=\"stock\",\n",
    "    symbols=[\"AAPL\", \"MSFT\"],\n",
    "    intervals=[\"1d\"],\n",
    "    sink=\"csv\",\n",
    "    first_date_override=\"20240101\",\n",
    "    end_date_override=\"20240131\",\n",
    "    discover_policy=DiscoverPolicy(mode=\"mild_skip\")\n",
    ")\n",
    "\n",
    "# Example C: wild - Discover symbols AND extend dates to present\n",
    "task_wild = Task(\n",
    "    asset=\"stock\",\n",
    "    symbols=[\"AAPL\"],\n",
    "    intervals=[\"1d\"],\n",
    "    sink=\"csv\",\n",
    "    discover_policy=DiscoverPolicy(mode=\"wild\")  # Will extend to current date\n",
    ")\n",
    "\n",
    "print(\"Discovery Policy Examples:\")\n",
    "print(f\"  - skip:      {task_skip.discover_policy.mode}\")\n",
    "print(f\"  - mild_skip: {task_mild.discover_policy.mode}\")\n",
    "print(f\"  - wild:      {task_wild.discover_policy.mode}\")\n",
    "print(\"\\nChoose one and run accordingly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex7'></a>\n",
    "## Example 7: Querying InfluxDB Data\n",
    "\n",
    "Query data written to InfluxDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires influxdb-client-3 library\n",
    "try:\n",
    "    from influxdb_client_3 import InfluxDBClient3\n",
    "    \n",
    "    # Configuration\n",
    "    client = InfluxDBClient3(\n",
    "        host=\"http://localhost:8086\",\n",
    "        token=\"your_influx_token_here\",\n",
    "        database=\"ThetaData\"\n",
    "    )\n",
    "    \n",
    "    # Query example: Get SPY data for December 2024\n",
    "    query = \"\"\"\n",
    "    SELECT time, open, high, low, close, volume\n",
    "    FROM SPY_stock_1d\n",
    "    WHERE time >= '2024-12-01T00:00:00Z'\n",
    "      AND time < '2025-01-01T00:00:00Z'\n",
    "    ORDER BY time ASC\n",
    "    \"\"\"\n",
    "    \n",
    "    # Uncomment to run (requires InfluxDB with data)\n",
    "    # table = client.query(query)\n",
    "    # df = table.to_pandas()\n",
    "    # print(f\"üìä Query returned {len(df)} rows\\n\")\n",
    "    # print(df.head())\n",
    "    # client.close()\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Example 7 requires InfluxDB with data. See Example 4 first.\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå influxdb-client-3 not installed. Run: pip install influxdb-client-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex8'></a>\n",
    "## Example 8: Verify Data Completeness\n",
    "\n",
    "Check for missing dates in downloaded CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Read downloaded CSV\n",
    "csv_file = \"./data/stock/AAPL/1d/csv/2024-01-01T00-00-00Z-AAPL-stock-1d_part01.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_file, dtype=str)\n",
    "    \n",
    "    # Parse timestamps\n",
    "    df['timestamp'] = pd.to_datetime(df['last_trade'], utc=True, errors='coerce')\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    \n",
    "    # Get date range\n",
    "    start_date = df['date'].min()\n",
    "    end_date = df['date'].max()\n",
    "    \n",
    "    # Generate expected trading days (excluding weekends)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='B')  # B = business days\n",
    "    expected_dates = set(date_range.date)\n",
    "    actual_dates = set(df['date'].unique())\n",
    "    \n",
    "    # Find missing dates\n",
    "    missing_dates = expected_dates - actual_dates\n",
    "    \n",
    "    print(f\"üìÖ Date Range: {start_date} to {end_date}\")\n",
    "    print(f\"üìä Total rows: {len(df)}\")\n",
    "    print(f\"‚úÖ Expected trading days: {len(expected_dates)}\")\n",
    "    print(f\"‚úÖ Actual trading days: {len(actual_dates)}\")\n",
    "    \n",
    "    if missing_dates:\n",
    "        print(f\"\\n‚ö†Ô∏è Missing {len(missing_dates)} dates:\")\n",
    "        for date in sorted(missing_dates)[:10]:  # Show first 10\n",
    "            print(f\"  - {date}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing dates! Data is complete.\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File not found: {csv_file}\")\n",
    "    print(\"Run Example 1 first to download data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ‚úÖ **Basic EOD download** - Single symbol to CSV\n",
    "2. ‚úÖ **Multi-symbol download** - Parallel processing\n",
    "3. ‚úÖ **Intraday data** - 5-minute bars to Parquet\n",
    "4. ‚úÖ **InfluxDB integration** - Time-series database\n",
    "5. ‚úÖ **Coherence checking** - Automatic gap detection\n",
    "6. ‚úÖ **Discovery policies** - Control symbol/date behavior\n",
    "7. ‚úÖ **Query InfluxDB** - Retrieve stored data\n",
    "8. ‚úÖ **Data validation** - Verify completeness\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Read the full manual: [MANUAL.md](../MANUAL.md)\n",
    "- Explore API reference: [Chapter 18](../MANUAL.md#18-api-reference)\n",
    "- See batch automation: [start_environment.bat](../start_environment.bat)\n",
    "\n",
    "---\n",
    "\n",
    "**Documentation:** https://github.com/fede72bari/tdSynchManager  \n",
    "**Issues:** https://github.com/fede72bari/tdSynchManager/issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
