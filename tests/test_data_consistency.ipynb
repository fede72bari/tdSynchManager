{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Consistency Testing - Comprehensive Test Suite\n",
    "\n",
    "Tests:\n",
    "1. Real-time download with validation and retry\n",
    "2. InfluxDB write verification\n",
    "3. Post-hoc coherence checking\n",
    "4. Recovery mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: HTTP Logging Wrapper\n",
    "\n",
    "def enable_http_logging_for_class(ClientClass):\n",
    "    import functools\n",
    "    from urllib.parse import urlencode\n",
    "\n",
    "    orig_make_request = ClientClass._make_request\n",
    "\n",
    "    @functools.wraps(orig_make_request)\n",
    "    async def wrapped(self, endpoint, params):\n",
    "        import time\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            resp_text, raw_url = await orig_make_request(self, endpoint, params)\n",
    "            dt = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "            base = raw_url or (self.base_url.rstrip(\"/\") + endpoint)\n",
    "            qs = urlencode({k: v for k, v in (params or {}).items() if v is not None})\n",
    "            full = f\"{base}?{qs}\" if qs else base\n",
    "\n",
    "            print(f\"[TD-HTTP] GET {full} -> OK in {dt:.1f} ms\")\n",
    "            return resp_text, raw_url\n",
    "        except Exception as e:\n",
    "            base = (getattr(self, \"base_url\", \"\") or \"\").rstrip(\"/\") + endpoint\n",
    "            qs = urlencode({k: v for k, v in (params or {}).items() if v is not None})\n",
    "            full = f\"{base}?{qs}\" if qs else base\n",
    "            print(f\"[TD-HTTP] GET {full} -> ERROR: {e}\")\n",
    "            raise\n",
    "\n",
    "    ClientClass._make_request = wrapped\n",
    "\n",
    "\n",
    "def enable_http_logging():\n",
    "    \"\"\"\n",
    "    Monkey-patch idempotent on aiohttp.ClientSession._request to log final URLs (with query).\n",
    "    If already patched, it won't patch twice.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import aiohttp\n",
    "    from yarl import URL\n",
    "\n",
    "    if getattr(aiohttp.ClientSession._request, \"_td_patched\", False):\n",
    "        return  # already patched\n",
    "\n",
    "    original = aiohttp.ClientSession._request\n",
    "\n",
    "    async def wrapped(self, method, path_or_url, *args, **kwargs):\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            res = await original(self, method, path_or_url, *args, **kwargs)\n",
    "            dt = (time.perf_counter() - t0) * 1000\n",
    "            try:\n",
    "                final_url = str(res.url)\n",
    "            except Exception:\n",
    "                q = kwargs.get(\"params\")\n",
    "                final_url = str(URL(path_or_url).with_query(q)) if q else str(path_or_url)\n",
    "            print(f\"[TD-HTTP] {method} {final_url} -> {res.status} in {dt:.1f} ms\")\n",
    "            return res\n",
    "        except Exception as e:\n",
    "            dt = (time.perf_counter() - t0) * 1000\n",
    "            try:\n",
    "                q = kwargs.get(\"params\")\n",
    "                final_url = str(URL(path_or_url).with_query(q)) if q else str(path_or_url)\n",
    "            except Exception:\n",
    "                final_url = str(path_or_url)\n",
    "            status = getattr(e, \"status\", \"\")\n",
    "            print(f\"[TD-HTTP] {method} {final_url} -> ERROR {status}: {e} in {dt:.1f} ms\")\n",
    "            raise\n",
    "\n",
    "    wrapped._td_patched = True\n",
    "    aiohttp.ClientSession._request = wrapped\n",
    "\n",
    "enable_http_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration and Task Setup\n\nimport os\nfrom src.tdSynchManager.credentials import get_influx_credentials\nfrom src.tdSynchManager.ThetaDataV3Client import ThetaDataV3Client\nfrom src.tdSynchManager.manager import ThetaSyncManager\nfrom src.tdSynchManager.config import ManagerConfig, Task, DiscoverPolicy\nfrom src.tdSynchManager.utils import install_td_server_error_logger\n\n# Get InfluxDB credentials\ninflux = get_influx_credentials()\ninflux_token = influx['token']\ninflux_url = influx.get('url', 'http://127.0.0.1:8181')\ninflux_bucket = influx.get('bucket', 'ThetaData')\n\nsymbols = [\"TLRY\"]  # Test symbol\n\ncfg = ManagerConfig(\n    root_dir=\"tests/data\",\n    max_concurrency=80,\n    max_file_mb=16,\n    overlap_seconds=60,\n    influx_url=influx_url,\n    influx_bucket=influx_bucket,\n    influx_token=influx_token,\n    influx_org=None,\n    influx_precision=\"nanosecond\",\n    influx_measure_prefix=\"\",\n    influx_write_batch=5000,\n    \n    # Enable validation with strict mode\n    enable_data_validation=True,\n    validation_strict_mode=True,  # All-or-nothing persistence\n)\n\ntasks = [\n    Task(\n        asset=\"option\",\n        symbols=symbols,\n        intervals=[\"1d\"],\n        sink=\"influxdb\",\n        enrich_bar_greeks=True,\n        enrich_tick_greeks=True,\n        first_date_override=\"20250821\",\n        ignore_existing=False,\n        discover_policy=DiscoverPolicy(mode=\"skip\")\n    ),\n    Task(\n        asset=\"option\",\n        symbols=symbols,\n        intervals=[\"5m\"],\n        sink=\"influxdb\",\n        enrich_bar_greeks=True,\n        enrich_tick_greeks=True,\n        first_date_override=\"20250821\",\n        ignore_existing=False,\n        discover_policy=DiscoverPolicy(mode=\"skip\")\n    ),\n]\n\nprint(\"Configuration loaded successfully\")\nprint(f\"Testing with symbols: {symbols}\")\nprint(f\"Validation enabled: {cfg.enable_data_validation}\")\nprint(f\"Strict mode: {cfg.validation_strict_mode}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Test Real-Time Download with Validation and Retry\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 1: Real-Time Download with Validation and Retry\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "async with ThetaDataV3Client() as client:\n",
    "    install_td_server_error_logger(client)\n",
    "    manager = ThetaSyncManager(cfg, client=client)\n",
    "    \n",
    "    print(\"\\n[TEST] Running download tasks...\")\n",
    "    print(\"Expected behavior:\")\n",
    "    print(\"  - Download data for TLRY options\")\n",
    "    print(\"  - Validate completeness (candles, columns, volume)\")\n",
    "    print(\"  - Retry up to N times if validation fails\")\n",
    "    print(\"  - Write to InfluxDB with verification\")\n",
    "    print(\"  - Retry only missing rows if partial write detected\")\n",
    "    print(\"\")\n",
    "    \n",
    "    await manager.run(tasks)\n",
    "    \n",
    "print(\"\\n[TEST] Download completed. Check logs above for:\")\n",
    "print(\"  ✓ [VALIDATION] messages showing validation results\")\n",
    "print(\"  ✓ [INFLUX][WRITE] messages showing write operations\")\n",
    "print(\"  ✓ Retry attempts if validation failed\")\n",
    "print(\"  ✓ Verification after write\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: List Available Measurements in InfluxDB\n",
    "\n",
    "from influxdb_client_3 import InfluxDBClient3\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Available InfluxDB Measurements\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "client = InfluxDBClient3(\n",
    "    host=influx_url,\n",
    "    database=influx_bucket,\n",
    "    token=influx_token\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Query to list all tables\n",
    "    query = \"SHOW TABLES\"\n",
    "    result = client.query(query)\n",
    "    df_tables = result.to_pandas()\n",
    "    \n",
    "    print(df_tables)\n",
    "    print(f\"\\nTotal measurements: {len(df_tables)}\")\n",
    "    \n",
    "    # Store for next cell\n",
    "    measurements = df_tables['table_name'].tolist()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error listing tables: {e}\")\n",
    "    measurements = []\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Test Post-Hoc Coherence Checking on All DBs\n\nfrom src.tdSynchManager.coherence import CoherenceChecker\nimport pandas as pd\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TEST 2: Post-Hoc Coherence Checking on All Measurements\")\nprint(\"=\"*80)\n\nasync with ThetaDataV3Client() as client:\n    install_td_server_error_logger(client)\n    manager = ThetaSyncManager(cfg, client=client)\n    \n    checker = CoherenceChecker(manager)\n    \n    # Test on all available measurements\n    for measurement in measurements:\n        print(f\"\\n{'='*60}\")\n        print(f\"Checking: {measurement}\")\n        print(f\"{'='*60}\")\n        \n        try:\n            # Parse measurement name: {symbol}-{asset}-{interval}\n            parts = measurement.split('-')\n            if len(parts) >= 3:\n                symbol = parts[0]\n                asset = parts[1]\n                interval = parts[2]\n                \n                # Determine date range to check (last 30 days)\n                end_date = pd.Timestamp.now().date()\n                start_date = end_date - pd.Timedelta(days=30)\n                \n                print(f\"Symbol: {symbol}, Asset: {asset}, Interval: {interval}\")\n                print(f\"Checking range: {start_date} to {end_date}\")\n                \n                # Run coherence check\n                issues = await checker.check_symbol_coherence(\n                    symbol=symbol,\n                    asset=asset,\n                    interval=interval,\n                    sink=\"influxdb\",\n                    start_date=str(start_date),\n                    end_date=str(end_date)\n                )\n                \n                if issues:\n                    print(f\"\\n⚠️  Found {len(issues)} issues:\")\n                    for issue in issues:\n                        print(f\"  - {issue.issue_type}: {issue.description}\")\n                        if 'problem_segments' in issue.details:\n                            for seg in issue.details['problem_segments'][:3]:  # Show first 3\n                                print(f\"    {seg.get('segment_start', 'N/A')}-{seg.get('segment_end', 'N/A')}: {seg.get('issue', 'N/A')}\")\n                else:\n                    print(\"✅ No coherence issues found\")\n                    \n        except Exception as e:\n            print(f\"❌ Error checking {measurement}: {e}\")\n            import traceback\n            traceback.print_exc()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Coherence check completed\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test InfluxDB Write Verification Directly\n",
    "\n",
    "from src.tdSynchManager.influx_verification import verify_influx_write, InfluxWriteResult\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: InfluxDB Write Verification (Direct Test)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create test DataFrame\n",
    "test_data = pd.DataFrame({\n",
    "    '__ts_utc': pd.date_range('2025-01-01', periods=10, freq='1H', tz='UTC'),\n",
    "    'symbol': ['TEST'] * 10,\n",
    "    'close': np.random.rand(10) * 100,\n",
    "    'volume': np.random.randint(1000, 10000, 10)\n",
    "})\n",
    "\n",
    "print(f\"\\nTest data created: {len(test_data)} rows\")\n",
    "print(test_data.head())\n",
    "\n",
    "# Connect to InfluxDB\n",
    "influx_client = InfluxDBClient3(\n",
    "    host=influx_url,\n",
    "    database=influx_bucket,\n",
    "    token=influx_token\n",
    ")\n",
    "\n",
    "measurement = \"test_verification\"\n",
    "\n",
    "print(f\"\\nWriting to measurement: {measurement}\")\n",
    "\n",
    "# Write test data using line protocol\n",
    "lines = []\n",
    "for _, row in test_data.iterrows():\n",
    "    ts_ns = int(row['__ts_utc'].value)\n",
    "    line = f\"{measurement},symbol={row['symbol']} close={row['close']},volume={row['volume']} {ts_ns}\"\n",
    "    lines.append(line)\n",
    "\n",
    "try:\n",
    "    influx_client.write(lines)\n",
    "    print(\"✅ Write completed\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Write failed: {e}\")\n",
    "\n",
    "# Verify write\n",
    "print(\"\\nVerifying write...\")\n",
    "\n",
    "import asyncio\n",
    "\n",
    "result = await verify_influx_write(\n",
    "    influx_client=influx_client,\n",
    "    measurement=measurement,\n",
    "    df_original=test_data,\n",
    "    key_cols=['__ts_utc', 'symbol'],\n",
    "    time_col='__ts_utc'\n",
    ")\n",
    "\n",
    "print(f\"\\nVerification result:\")\n",
    "print(f\"  Total attempted: {result.total_attempted}\")\n",
    "print(f\"  Successfully written: {result.successfully_written}\")\n",
    "print(f\"  Missing count: {result.missing_count}\")\n",
    "print(f\"  Success: {result.success}\")\n",
    "\n",
    "if result.missing_count > 0:\n",
    "    print(f\"\\n⚠️  Missing row indices: {result.missing_indices}\")\n",
    "else:\n",
    "    print(\"\\n✅ All rows verified successfully\")\n",
    "\n",
    "influx_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Check Data Consistency Logs\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data Consistency Logs\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "log_dir = os.path.join(cfg.root_dir, \"logs\")\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    log_files = glob.glob(os.path.join(log_dir, \"data_consistency_*.parquet\"))\n",
    "    \n",
    "    if log_files:\n",
    "        print(f\"Found {len(log_files)} log files:\\n\")\n",
    "        \n",
    "        # Read and display most recent log\n",
    "        latest_log = max(log_files, key=os.path.getmtime)\n",
    "        print(f\"Latest log: {os.path.basename(latest_log)}\")\n",
    "        \n",
    "        try:\n",
    "            log_df = pd.read_parquet(latest_log)\n",
    "            print(f\"\\nLog entries: {len(log_df)}\")\n",
    "            print(\"\\nRecent log entries:\")\n",
    "            print(log_df.tail(20).to_string())\n",
    "            \n",
    "            # Summary by event type\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"Summary by event type:\")\n",
    "            print(log_df['event_type'].value_counts())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading log: {e}\")\n",
    "    else:\n",
    "        print(\"No log files found\")\n",
    "else:\n",
    "    print(f\"Log directory not found: {log_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All tests completed\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}